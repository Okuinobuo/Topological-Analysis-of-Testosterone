import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Step 1: Load Patient Data (Replace this part with your actual data loading mechanism)
# Creating a sample DataFrame with clinical indicators for demonstration
np.random.seed(42)
num_patients = 100
patient_df = pd.DataFrame({
    'Age': np.random.randint(20, 80, num_patients),
    'Height_cm': np.random.uniform(150, 190, num_patients),
    'Weight_kg': np.random.uniform(50, 100, num_patients),
    'BMI': np.random.uniform(18, 35, num_patients),
    'Hb': np.random.uniform(12, 18, num_patients),
    'Hct': np.random.uniform(35, 50, num_patients),
    'Alb': np.random.uniform(3.5, 5.5, num_patients),
    'AST': np.random.uniform(10, 40, num_patients),
    'ALT': np.random.uniform(7, 56, num_patients),
    'ALP': np.random.uniform(30, 120, num_patients),
    'Cre': np.random.uniform(0.6, 1.3, num_patients),
    'Glu': np.random.uniform(70, 100, num_patients),
    'TG': np.random.uniform(50, 150, num_patients),
    'HDL': np.random.uniform(40, 60, num_patients),
    'LDL': np.random.uniform(70, 130, num_patients),
    'LH': np.random.uniform(1, 10, num_patients),
    'FSH': np.random.uniform(1, 10, num_patients),
    'CRP': np.random.uniform(0, 1, num_patients),
    'TT': np.random.uniform(300, 1000, num_patients)
})

# Step 2: Standardize the Data using Z-scores
scaler = StandardScaler()
standardized_data = scaler.fit_transform(patient_df)

# Step 3: Determine the Optimal Number of Clusters using the Elbow Method
sse = []  # List to store the sum of squared errors for each k
K_range = range(1, 11)  # Define the range of k values to test

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(standardized_data)
    sse.append(kmeans.inertia_)  # Inertia is the sum of squared errors within clusters

# Step 4: Plot the Elbow Graph
plt.figure(figsize=(10, 6))
plt.plot(K_range, sse, marker='o', linestyle='-', color='orange')
plt.title('Elbow Method for Optimal Cluster Number', fontsize=16)
plt.xlabel('Number of Clusters', fontsize=12)
plt.ylabel('Sum of Squared Errors (SSE)', fontsize=12)
plt.xticks(K_range)
plt.grid(True)
plt.show()

# Step 5: Apply K-means Clustering with the Optimal Number of Clusters (K=4 as identified from the elbow plot)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
patient_df['Cluster'] = kmeans.fit_predict(standardized_data)

# Display the clustered DataFrame
print(patient_df.head())

# Step 6: Visualize Clustering in 2D Space (using first two principal components for simplicity)
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_components = pca.fit_transform(standardized_data)

plt.figure(figsize=(12, 8))
plt.scatter(pca_components[:, 0], pca_components[:, 1], c=patient_df['Cluster'], cmap='viridis', marker='o')
plt.title('K-means Clustering Visualization (PCA-reduced)', fontsize=16)
plt.xlabel('Principal Component 1', fontsize=12)
plt.ylabel('Principal Component 2', fontsize=12)
plt.colorbar(label='Cluster Label')
plt.grid(True)
plt.show()
